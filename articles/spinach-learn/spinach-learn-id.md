<!-- title: Real-Time Spinach Detector â€” Pelajaran Mahal dari Deploy YOLO di Server Kecil -->
<!-- excerpt: Catatan teknis dan reflektif dari membangun aplikasi deteksi bayam berbasis YOLO (YOLOv9 & YOLOv11) â€” mulai dari Docker image yang terlalu besar, server overload, eksperimen ONNX yang gagal, hingga keputusan realistis untuk tetap berjalan secara lokal. -->
<!-- image: https://github.com/user-attachments/assets/1b008abf-73c5-4235-ac01-d63ad44c886e -->
<!-- date: 2025-12-17 -->
<!-- posting_date: 2025-12-17 -->
<!-- tags: Computer Vision, YOLO, Object Detection, ONNX, PyTorch, Docker, Deployment, Machine Learning, Web AI -->

# ğŸ¥¬ Real-Time Spinach Detector  
## Pelajaran Mahal dari Membangun YOLO untuk Deteksi Realtime di Web

<img width="1383" height="899" alt="image" src="https://github.com/user-attachments/assets/1b008abf-73c5-4235-ac01-d63ad44c886e" />

Project **Spinach Detector** ini awalnya terlihat sederhana:  
ğŸ‘‰ *â€œDeteksi apakah gambar atau kamera berisi bayam atau bukan.â€*

Namun dalam praktiknya, project ini berubah menjadi **pelajaran nyata tentang batasan resource, deployment ML, dan realitas object detection realtime di web**.

Repo ini **tidak berakhir dengan deployment cloud yang sempurna**,  
tapi justru **berakhir dengan pemahaman yang jauh lebih matang**.

---

## ğŸ¯ Tujuan Awal Project

Tujuan utama project ini sebenarnya **bukan production**:

- demonstrasi object detection berbasis YOLO
- realtime detection via kamera (web)
- keperluan demo dan presentasi di kampus
- eksplorasi YOLOv9 dan YOLOv11

Secara fungsional:
- backend: Flask + YOLO
- frontend: Web camera + canvas overlay
- model: YOLOv9 & YOLOv11 (custom dataset bayam)

---

## ğŸ§± Masalah Pertama: Docker Image Terlalu Besar

Saat mencoba membungkus aplikasi ke dalam Docker, masalah pertama langsung muncul.

### âŒ Docker Image = Â±4GB

Penyebab utamanya:
- model `.pt` YOLO
- PyTorch + CUDA dependency
- OpenCV, numpy, pillow
- dependency ML yang **sangat berat**

Dampaknya:
- `docker pull` **lama sekali**
- tidak realistis untuk server kecil
- deployment jadi tidak efisien

Untuk aplikasi web biasa, ini mungkin masih bisa ditoleransi.  
Tapi untuk **ML inference realtime**, ini sudah red flag.

---

## ğŸ–¥ï¸ Masalah Kedua: Server Kecil Tidak Kuat Menangani Realtime Detection

Server yang digunakan:
- CPU kecil
- RAM terbatas
- tanpa GPU

Ketika aplikasi mulai dijalankan dan:
- kamera aktif
- inference YOLO berjalan terus-menerus
- request datang tiap beberapa ratus milidetik

â¡ï¸ **Server langsung overload**  
â¡ï¸ CPU spike  
â¡ï¸ RAM penuh  
â¡ï¸ proses mati  
â¡ï¸ server harus reboot berulang kali  

Pada titik ini, jelas bahwa:
> **Realtime object detection + server kecil = kombinasi yang buruk**

---

## ğŸ”„ Eksperimen: Migrasi dari `.pt` ke ONNX

Untuk mengurangi beban server, dicoba pendekatan lain:

### ğŸ¯ Ide:
- konversi YOLO `.pt` â†’ `.onnx`
- harapan: runtime lebih ringan
- dependency lebih sedikit
- Docker image lebih kecil

### âŒ Realita:
- hasil deteksi **aneh**
- bounding box tidak akurat
- multiple detection untuk satu objek
- koordinat kacau
- perbedaan besar antara PyTorch vs ONNX output

Masalah utama:
- ONNX **tidak auto-NMS**
- format output YOLO ONNX berbeda
- preprocessing & postprocessing harus ditulis manual
- satu kesalahan kecil â†’ hasil langsung rusak

Alih-alih menyederhanakan, ONNX justru:
> **menambah kompleksitas dan risiko bug**

---

## ğŸ”™ Kembali ke Awal: Tetap Pakai `.pt`, Jalan Lokal

Setelah beberapa iterasi dan eksperimen, keputusan akhirnya diambil:

- âŒ tidak pakai Docker
- âŒ tidak deploy ke server
- âŒ tidak realtime via cloud
- âœ… pakai YOLO `.pt`
- âœ… jalan **secara lokal**
- âœ… cukup untuk demo dan pembelajaran

Dan untuk konteks project ini,  
**keputusan itu adalah keputusan yang paling masuk akal.**

---

## ğŸ§  Pelajaran Penting dari Project Ini

### 1ï¸âƒ£ Realtime Object Detection Itu Mahal

Deteksi realtime via kamera:
- bukan cuma soal model
- tapi soal:
  - CPU
  - RAM
  - IO
  - concurrency
  - frame rate
  - request frequency

Ini **bukan workload ringan**.

---

### 2ï¸âƒ£ ML di Server â‰  ML di Device

Perbedaan besar:
- **server inference realtime** â†’ resource heavy
- **model di device (mobile / edge)** â†’ jauh lebih efisien

Makanya:
- aplikasi kamera di HP terasa ringan
- tapi server inference cepat overload

Karena:
> **model ditanam langsung di device, bukan dipanggil via HTTP terus-menerus**

---

### 3ï¸âƒ£ ONNX Bukan Obat Mujarab

ONNX itu powerful, tapi:
- tidak plug-and-play
- butuh pemahaman format output
- NMS harus manual
- preprocessing harus presisi

Tanpa itu:
> hasil bisa â€œjalanâ€, tapi **secara logika salah**

---

### 4ï¸âƒ£ Web + Realtime AI = Banyak Layer Mahal

Realtime AI di web bukan cuma soal backend:
- kamera di browser
- canvas rendering
- request loop
- latency
- bandwidth
- resource server

Semua itu **numpuk biayanya**.

---

## ğŸ§¾ Kesimpulan

Project **Spinach Detector** ini mungkin tidak berakhir dengan deployment cloud yang stabil,  
tapi justru menghasilkan sesuatu yang lebih berharga:

> **pemahaman realistis tentang batasan object detection realtime di web.**

Tidak semua project harus:
- scalable
- cloud-ready
- production-grade

Kadang:
- **cukup jalan lokal**
- **cukup untuk demo**
- **cukup untuk belajar**

Dan knowing **kapan harus berhenti optimasi**  
adalah bagian penting dari menjadi engineer yang matang.

---

## ğŸ”— Resource & Link

- **GitHub Repo**  
  https://github.com/daffa09/spinach-detector

- **Google Colab â€” YOLOv9 Training**  
  https://colab.research.google.com/drive/1F43i2TkWXIefNw2KuiO1pMnAmc4pKmuZ?usp=sharing

- **Google Colab â€” YOLOv11 Training**  
  https://colab.research.google.com/drive/1ahSpgDHbQJqJuPKEcyajPbtrDvT1P-3I?usp=sharing

---

## ğŸ§  Penutup

Project ini bukan tentang â€œdeteksi bayamâ€.  
Project ini tentang **belajar batasan sistem nyata**.

Dan itu pelajaran yang jauh lebih mahal daripada sekadar model yang akurat.
